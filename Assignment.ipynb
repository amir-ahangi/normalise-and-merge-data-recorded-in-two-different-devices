{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "# Load the pickle file\n",
    "data = pd.read_pickle('two_devices_data.pkl')\n",
    "\n",
    "\n",
    "# Inspect the keys of the dictionary\n",
    "print(data.keys())\n",
    "\n",
    "# Assuming the dictionary has keys: 'Probe', 'Backscat', 'Date', 'Label'\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Display the structure and summary of the dataset\n",
    "df.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Label' column to string if necessary\n",
    "if isinstance(df['Label'].iloc[0], np.ndarray):\n",
    "    df['Label'] = df['Label'].apply(lambda x: str(x))\n",
    "\n",
    "# Verify the conversion\n",
    "print(df['Label'].head())\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df['Label'] = df['Label'].str.strip(\"[]'\")\n",
    "\n",
    "print(df['Label'].unique())\n",
    "\n",
    "\n",
    "\n",
    "def plot_signals_with_plotly(data, sample_label, num_signals=2):\n",
    "    # Filter the data for the given label\n",
    "    sample_data = data[data['Label'] == sample_label]\n",
    "\n",
    "    if len(sample_data) < num_signals:\n",
    "        print(f\"Not enough data to plot for label {sample_label}. Available samples: {len(sample_data)}\")\n",
    "        return\n",
    "\n",
    "    # Process and plot each signal\n",
    "    for i, row in sample_data.head(num_signals).iterrows():\n",
    "        # Check if 'Backscat' is a NumPy array and handle it\n",
    "        if isinstance(row['Backscat'], np.ndarray):\n",
    "            # Check the dimension of the array\n",
    "            if row['Backscat'].ndim == 2:\n",
    "             \n",
    "                backscat_data = row['Backscat'][0, :]  # Assume each row is a recording\n",
    "                print(f\"Using one recording of length {len(backscat_data)} for plotting.\")\n",
    "            else:\n",
    "                print(f\"Backscat array has unexpected number of dimensions: {row['Backscat'].ndim}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"Unexpected data type in Backscat: {type(row['Backscat'])}\")\n",
    "            continue\n",
    "\n",
    "        # Create the Plotly figure\n",
    "        fig = px.line(\n",
    "            x=np.arange(len(backscat_data)), \n",
    "            y=backscat_data,\n",
    "            title=f'Signal for {sample_label} - Probe: {row[\"Probe\"]}',\n",
    "            labels={'x': 'Index', 'y': 'Backscatter Intensity'}\n",
    "        )\n",
    "        \n",
    "        # Display the figure\n",
    "        fig.show()\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's been properly formatted\n",
    "plot_signals_with_plotly(df, 'C2', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define a function to extract features from Backscat\n",
    "def extract_features(backscat_array):\n",
    "    features = {\n",
    "        'mean': np.mean(backscat_array, axis=1),\n",
    "        'std': np.std(backscat_array, axis=1),\n",
    "        'max': np.max(backscat_array, axis=1),\n",
    "        'min': np.min(backscat_array, axis=1),\n",
    "        'median': np.median(backscat_array, axis=1),\n",
    "        '25_percentile': np.percentile(backscat_array, 25, axis=1),\n",
    "        '50_percentile': np.percentile(backscat_array, 50, axis=1),\n",
    "        '75_percentile': np.percentile(backscat_array, 75, axis=1),\n",
    "    }\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Extract features for the entire dataset\n",
    "df_features_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    backscat_features = extract_features(row['Backscat'])\n",
    "    backscat_features['Label'] = row['Label']\n",
    "    df_features_list.append(backscat_features)\n",
    "\n",
    "# Combine all the features into a single DataFrame\n",
    "df_features = pd.concat(df_features_list, ignore_index=True)\n",
    "\n",
    "# Prepare the data for ML\n",
    "X = df_features.drop(columns=['Label'])\n",
    "y = df_features['Label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.linalg import sqrtm\n",
    "import pickle\n",
    "\n",
    "data=df\n",
    "\n",
    "# Function to extract frequency domain features using Welch's method\n",
    "def extract_features(signal):\n",
    "    freqs, psd = welch(signal)\n",
    "    return np.array([psd.mean(), np.max(psd), np.std(psd)])\n",
    "\n",
    "# Apply feature extraction to each 'Backscat' entry\n",
    "data['Features'] = data['Backscat'].apply(lambda x: extract_features(np.array(x)))\n",
    "\n",
    "# Prepare data\n",
    "features = np.vstack(data['Features'].values)\n",
    "labels = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Split the data by Probe\n",
    "probe1_data = data[data['Probe'] == 'OZ02']\n",
    "probe2_data = data[data['Probe'] == 'OZ03']\n",
    "\n",
    "X_probe1 = np.vstack(probe1_data['Features'].values)\n",
    "X_probe2 = np.vstack(probe2_data['Features'].values)\n",
    "y_probe1 = LabelEncoder().fit_transform(probe1_data['Label'])\n",
    "y_probe2 = LabelEncoder().fit_transform(probe2_data['Label'])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_probe1_scaled = scaler.fit_transform(X_probe1)\n",
    "X_probe2_scaled = scaler.transform(X_probe2)\n",
    "\n",
    "# Function to align data using CORAL\n",
    "def coral(source, target):\n",
    "    source_cov = np.cov(source.T) + 1e-5 * np.eye(source.shape[1])\n",
    "    target_cov = np.cov(target.T) + 1e-5 * np.eye(target.shape[1])\n",
    "    source_whiten = sqrtm(source_cov)\n",
    "    target_color = sqrtm(target_cov)\n",
    "    source_aligned = source @ np.linalg.inv(source_whiten) @ target_color\n",
    "    return source_aligned\n",
    "\n",
    "# Align Probe1 data to Probe2\n",
    "X_probe1_aligned = coral(X_probe1_scaled, X_probe2_scaled)\n",
    "\n",
    "# Classification with Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_probe1_aligned, y_probe1)\n",
    "y_pred = model.predict(X_probe2_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy on Probe2 data:\", accuracy_score(y_probe2, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_probe2, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import sqrtm\n",
    "import pickle\n",
    "\n",
    "data=df\n",
    "\n",
    "# Function to extract features using Wavelet Transform\n",
    "def extract_wavelet_features(signal):\n",
    "    coeffs = pywt.wavedec(signal, wavelet='db4', level=3)\n",
    "    features = []\n",
    "    for coeff in coeffs:\n",
    "        features.extend([np.mean(coeff), np.std(coeff), np.max(coeff)])\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to each 'Backscat' entry\n",
    "data['Wavelet_Features'] = data['Backscat'].apply(lambda x: extract_wavelet_features(np.array(x)))\n",
    "\n",
    "# Prepare features and labels\n",
    "features = np.vstack(data['Wavelet_Features'])\n",
    "labels = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Split the data by Probe\n",
    "probe1_data = data[data['Probe'] == 'OZ02']\n",
    "probe2_data = data[data['Probe'] == 'OZ03']\n",
    "\n",
    "X_probe1 = np.vstack(probe1_data['Wavelet_Features'])\n",
    "X_probe2 = np.vstack(probe2_data['Wavelet_Features'])\n",
    "y_probe1 = LabelEncoder().fit_transform(probe1_data['Label'])\n",
    "y_probe2 = LabelEncoder().fit_transform(probe2_data['Label'])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_probe1_scaled = scaler.fit_transform(X_probe1)\n",
    "X_probe2_scaled = scaler.transform(X_probe2)\n",
    "\n",
    "# Function to align data using CORAL\n",
    "def coral(source, target):\n",
    "    source_cov = np.cov(source.T) + 1e-5 * np.eye(source.shape[1])\n",
    "    target_cov = np.cov(target.T) + 1e-5 * np.eye(target.shape[1])\n",
    "    source_whiten = sqrtm(source_cov)\n",
    "    target_color = sqrtm(target_cov)\n",
    "    source_aligned = source @ np.linalg.inv(source_whiten) @ target_color\n",
    "    return source_aligned\n",
    "\n",
    "# Align Probe1 data to Probe2\n",
    "X_probe1_aligned = coral(X_probe1_scaled, X_probe2_scaled)\n",
    "\n",
    "# Classification with Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_probe1_aligned, y_probe1)\n",
    "y_pred = model.predict(X_probe2_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy on Probe2 data:\", accuracy_score(y_probe2, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_probe2, y_pred, target_names=np.unique(data['Label'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pickle\n",
    "\n",
    "data=df\n",
    "\n",
    "# Feature extraction functions\n",
    "def extract_time_features(signal):\n",
    "    return [np.mean(signal), np.std(signal), np.min(signal), np.max(signal)]\n",
    "\n",
    "def extract_frequency_features(signal):\n",
    "    freqs, psd = welch(signal)\n",
    "    return [np.mean(psd), np.max(psd), np.std(psd)]\n",
    "\n",
    "def extract_wavelet_features(signal):\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=3)\n",
    "    features = []\n",
    "    for coeff in coeffs:\n",
    "        features.extend([np.mean(coeff), np.std(coeff), np.max(coeff)])\n",
    "    return features\n",
    "\n",
    "# Applying feature extraction\n",
    "data['Time_Features'] = data['Backscat'].apply(lambda x: extract_time_features(np.array(x)))\n",
    "data['Frequency_Features'] = data['Backscat'].apply(lambda x: extract_frequency_features(np.array(x)))\n",
    "data['Wavelet_Features'] = data['Backscat'].apply(lambda x: extract_wavelet_features(np.array(x)))\n",
    "\n",
    "# Split data by Probe\n",
    "probe1_data = data[data['Probe'] == 'OZ02']\n",
    "probe2_data = data[data['Probe'] == 'OZ03']\n",
    "\n",
    "# Scale features\n",
    "scaler_time = StandardScaler()\n",
    "scaler_freq = StandardScaler()\n",
    "scaler_wave = StandardScaler()\n",
    "\n",
    "X_time1 = scaler_time.fit_transform(np.vstack(probe1_data['Time_Features']))\n",
    "X_freq1 = scaler_freq.fit_transform(np.vstack(probe1_data['Frequency_Features']))\n",
    "X_wave1 = scaler_wave.fit_transform(np.vstack(probe1_data['Wavelet_Features']))\n",
    "\n",
    "X_time2 = scaler_time.transform(np.vstack(probe2_data['Time_Features']))\n",
    "X_freq2 = scaler_freq.transform(np.vstack(probe2_data['Frequency_Features']))\n",
    "X_wave2 = scaler_wave.transform(np.vstack(probe2_data['Wavelet_Features']))\n",
    "\n",
    "y_probe1 = LabelEncoder().fit_transform(probe1_data['Label'])\n",
    "y_probe2 = LabelEncoder().fit_transform(probe2_data['Label'])\n",
    "\n",
    "# Setup classifiers\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "svc = SVC(probability=True, kernel='rbf')\n",
    "\n",
    "# Train each classifier\n",
    "rf.fit(X_time1, y_probe1)\n",
    "gb.fit(X_freq1, y_probe1)\n",
    "svc.fit(X_wave1, y_probe1)\n",
    "\n",
    "# Evaluate each classifier\n",
    "y_pred_rf = rf.predict(X_time2)\n",
    "y_pred_gb = gb.predict(X_freq2)\n",
    "y_pred_svc = svc.predict(X_wave2)\n",
    "\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_probe2, y_pred_rf))\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_probe2, y_pred_rf, zero_division=0))\n",
    "\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy_score(y_probe2, y_pred_gb))\n",
    "print(\"Gradient Boosting Classification Report:\\n\", classification_report(y_probe2, y_pred_gb, zero_division=0))\n",
    "\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_probe2, y_pred_svc))\n",
    "print(\"SVC Classification Report:\\n\", classification_report(y_probe2, y_pred_svc, zero_division=0))\n",
    "\n",
    "# Calculate agreement and disagreement\n",
    "def calculate_agreement(y_pred1, y_pred2, y_pred3):\n",
    "    total_samples = len(y_pred1)\n",
    "    all_agree = np.sum((y_pred1 == y_pred2) & (y_pred2 == y_pred3))\n",
    "    pairwise_disagree_1_2 = np.sum(y_pred1 != y_pred2)\n",
    "    pairwise_disagree_1_3 = np.sum(y_pred1 != y_pred3)\n",
    "    pairwise_disagree_2_3 = np.sum(y_pred2 != y_pred3)\n",
    "    all_disagree = np.sum((y_pred1 != y_pred2) & (y_pred1 != y_pred3) & (y_pred2 != y_pred3))\n",
    "    \n",
    "    agreement_rate = all_agree / total_samples\n",
    "    disagreement_rate_1_2 = pairwise_disagree_1_2 / total_samples\n",
    "    disagreement_rate_1_3 = pairwise_disagree_1_3 / total_samples\n",
    "    disagreement_rate_2_3 = pairwise_disagree_2_3 / total_samples\n",
    "    all_disagreement_rate = all_disagree / total_samples\n",
    "    \n",
    "    return agreement_rate, disagreement_rate_1_2, disagreement_rate_1_3, disagreement_rate_2_3, all_disagreement_rate\n",
    "\n",
    "# Calculate agreement and disagreement rates\n",
    "agreement_rate, disagreement_rate_1_2, disagreement_rate_1_3, disagreement_rate_2_3, all_disagreement_rate = calculate_agreement(y_pred_rf, y_pred_gb, y_pred_svc)\n",
    "\n",
    "print(\"Agreement Rate:\", agreement_rate)\n",
    "print(\"Disagreement Rate between RF and GB:\", disagreement_rate_1_2)\n",
    "print(\"Disagreement Rate between RF and SVC:\", disagreement_rate_1_3)\n",
    "print(\"Disagreement Rate between GB and SVC:\", disagreement_rate_2_3)\n",
    "print(\"Disagreement Rate among all three classifiers:\", all_disagreement_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "\n",
    "data=df\n",
    "\n",
    "# Feature extraction functions\n",
    "def extract_time_features(signal):\n",
    "    return [np.mean(signal), np.std(signal), np.min(signal), np.max(signal)]\n",
    "\n",
    "def extract_frequency_features(signal):\n",
    "    freqs, psd = welch(signal)\n",
    "    return [np.mean(psd), np.max(psd), np.std(psd)]\n",
    "\n",
    "def extract_wavelet_features(signal):\n",
    "    coeffs = pywt.wavedec(signal, 'db4', level=3)\n",
    "    features = []\n",
    "    for coeff in coeffs:\n",
    "        features.extend([np.mean(coeff), np.std(coeff), np.max(coeff)])\n",
    "    return features\n",
    "\n",
    "# Applying feature extraction\n",
    "data['Time_Features'] = data['Backscat'].apply(lambda x: extract_time_features(np.array(x)))\n",
    "data['Frequency_Features'] = data['Backscat'].apply(lambda x: extract_frequency_features(np.array(x)))\n",
    "data['Wavelet_Features'] = data['Backscat'].apply(lambda x: extract_wavelet_features(np.array(x)))\n",
    "\n",
    "# Prepare data\n",
    "labels = LabelEncoder().fit_transform(data['Label'])\n",
    "\n",
    "# Split data by Probe\n",
    "probe1_data = data[data['Probe'] == 'OZ02']\n",
    "probe2_data = data[data['Probe'] == 'OZ03']\n",
    "\n",
    "# Separate and scale features\n",
    "def scale_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(np.vstack(features)), scaler\n",
    "\n",
    "X_time1, scaler_time = scale_features(probe1_data['Time_Features'])\n",
    "X_freq1, scaler_freq = scale_features(probe1_data['Frequency_Features'])\n",
    "X_wave1, scaler_wave = scale_features(probe1_data['Wavelet_Features'])\n",
    "\n",
    "X_time2 = scaler_time.transform(np.vstack(probe2_data['Time_Features']))\n",
    "X_freq2 = scaler_freq.transform(np.vstack(probe2_data['Frequency_Features']))\n",
    "X_wave2 = scaler_wave.transform(np.vstack(probe2_data['Wavelet_Features']))\n",
    "\n",
    "# Prepare labels\n",
    "y_probe1 = LabelEncoder().fit_transform(probe1_data['Label'])\n",
    "y_probe2 = LabelEncoder().fit_transform(probe2_data['Label'])\n",
    "\n",
    "# Setup classifiers\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "svc = SVC(probability=True, kernel='rbf')\n",
    "\n",
    "# Fit each model on different feature sets\n",
    "rf.fit(X_time1, y_probe1)\n",
    "gb.fit(X_freq1, y_probe1)\n",
    "svc.fit(X_wave1, y_probe1)\n",
    "\n",
    "# Create a VotingClassifier for ensemble learning\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('gb', gb),\n",
    "    ('svc', svc)\n",
    "], voting='soft')\n",
    "\n",
    "# Evaluate on combined feature set from Probe2\n",
    "X_combined2 = np.hstack([X_time2, X_freq2, X_wave2])\n",
    "ensemble.fit(X_combined2, y_probe2)\n",
    "\n",
    "# Prediction and Evaluation\n",
    "y_pred = ensemble.predict(X_combined2)\n",
    "accuracy = accuracy_score(y_probe2, y_pred)\n",
    "print(\"Enhanced Accuracy on Probe2 data:\", accuracy)\n",
    "print(\"Enhanced Classification Report:\\n\", classification_report(y_probe2, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
